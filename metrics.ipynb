{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [\n",
    "    [1, 3, 1, 0],\n",
    "    [1, 3, 1, 0],\n",
    "    [1, 3, 1, 0],\n",
    "    [1, 4, 1, 0],\n",
    "    [1, 4, 1, 0],\n",
    "    [1, 4, 1, 0],\n",
    "    [1, 4, 1, 0],\n",
    "    [1, 7, 2, 1],\n",
    "    [1, 7, 2, 1],\n",
    "    [1, 7, 2, 1]\n",
    "]\n",
    "#y_true = np.ravel(y_true)\n",
    "y_true = np.array(y_true)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [1, 4, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [1, 4, 0, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [\n",
    "    [0, 2, 0, 0],\n",
    "    [0, 2, 0, 0],\n",
    "    [0, 2, 0, 0],\n",
    "    [0, 4, 0, 0],\n",
    "    [0, 4, 0, 0],\n",
    "    [0, 4, 0, 0],\n",
    "    [0, 4, 0, 0],\n",
    "    [1, 4, 0, 0],\n",
    "    [0, 4, 0, 0],\n",
    "    [1, 4, 0, 0]\n",
    "]\n",
    "#y_pred = np.ravel(y_pred)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for i in range(0, len(y_true)):\n",
    "    for j in range(0, 4):\n",
    "        if y_true[i][j] > 0:\n",
    "            if y_true[i][j] >= y_pred[i][j]:\n",
    "                diff = abs(y_true[i][j] - y_pred[i][j])\n",
    "                tp += y_pred[i][j]\n",
    "                fn += diff\n",
    "            else:\n",
    "                diff = abs(y_true[i][j] - y_pred[i][j])\n",
    "                tp += y_true[i][j]\n",
    "                fp += diff\n",
    "        elif y_true[i][j] == 0:\n",
    "            if y_true[i][j] == y_pred[i][j]:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fp += y_pred[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr = tp/(tp+fn)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnr = tn/(tn+fp) \n",
    "tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr = fp/(fp+tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnr = fn/(tp+fn)\n",
    "fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 0, 36, 7]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = [tp, fp, fn, tn]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = []\n",
    "for i in range(4):\n",
    "    accuracy.append((cm[i][0]+cm[i][3])/sum(cm[i]) if sum(cm[i])!=0 else 0.0)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.85, 0.0, 0.0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = []\n",
    "for i in range(4):\n",
    "    precision.append((cm[i][0])/(cm[i][0]+cm[i][1]) if cm[i][0]+cm[i][1]!=0 else 0.0)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = []\n",
    "for i in range(4):\n",
    "    recall.append((cm[i][0])/(cm[i][0]+cm[i][2]) if cm[i][0]+cm[i][2]!=0 else 0.0)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33333333333333337, 0.9189189189189189, 0.0, 0.0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1score = []\n",
    "for i in range(4):\n",
    "    f1score.append((2*precision[i]*recall[i])/(precision[i]+recall[i]) if precision[i]+recall[i]!=0 else 0.0)\n",
    "f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         7\n",
      "           1       1.00      0.10      0.18        20\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.57      1.00      0.73         4\n",
      "           7       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.33        40\n",
      "   macro avg       0.30      0.35      0.22        40\n",
      "weighted avg       0.60      0.33      0.23        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prana\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prana\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = np.array(['Swipe_machine', 'Product', 'Person', 'Head'])\n",
    "classification_rep = classification_report(np.ravel(y_true), np.ravel(y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
